<!-- Research Section -->
<section class="py-20 bg-white">
  <div class="container mx-auto px-6">
    <div class="max-w-4xl mx-auto">
      <div class="prose prose-lg text-gray-700">
        <h2 class="text-2xl font-serif font-bold mb-12 text-primary">research interests</h2>
        <p>
          I'm interested in exploring methodologies that help us understand and bridge the socio-technical gap in AI systems—particularly in how they are <strong>evaluated, deployed,</strong> and <strong>governed</strong>.
        </p>

        <br>
        <p>Currently thinking about:</p>
        <p>&nbsp;</p>
        <ul class="list-disc pl-5 space-y-3">
          <li><strong>Improving Evaluations</strong> e.g. <em>How can we better measure evaluations and datasets to ensure construct and claim validity?</em></li>
          <li><strong>Context-specificity</strong> e.g. <em>How do we operationalize evaluations in multilingual and code-switching environments?</em></li>
          <li><strong>Performance Robustness</strong> e.g. <em>To what extent are model behaviors stable—and hence evaluation results robust—across perturbations, shifts, or rephrasings?</em></li>
        </ul>
        <p>&nbsp;</p>
        <p>If you're also thinking about the above, let's chat.</p>

        <div class="my-8 flex justify-center">
          <div class="w-24 h-1 bg-gradient-to-r from-primary to-accent rounded-full"></div>
        </div>

        <h2 class="text-2xl font-serif font-bold mb-12 text-primary">selected projects & work</h2>

        <ul class="space-y-8">
          <li>
            <h3 class="font-semibold text-lg text-primary"><a href="https://www.taig-icml.com/home" class="text-accent hover:underline" target="_blank">CALMA: Context-Aligned Language Model Alignment</a></h3>
            <p class="text-sm italic">Prajna Soni, Deepika Raman, Dylan Hadfield-Menell — Preprint @ ICML 2025 Technical AI Governance Workshop <b>** Oral Presentation **</b></p>
            <p>A framework for deriving context-specific values and alignment axes for language models by leveraging social research theories.</p>
          </li>

          <li>
            <h3 class="font-semibold text-lg text-primary"><a href="https://dspace.mit.edu/handle/1721.1/156962" class="text-accent hover:underline" target="_blank">Addressing Misalignment in Language Model Deployments through Context-Specific Evaluations</a></h3>
            <p class="text-sm italic">Prajna Soni — SM Thesis @ MIT</p>
            <p>Investigates technical and regulatory methods to assess and mitigate deployment-time misalignment of LLMs.</p>
          </li>

          <li>
            <h3 class="font-semibold text-lg text-primary"><a href="https://amulyayadav.github.io/AI4SG2023/" class="text-accent hover:underline" target="_blank">FARE: Fair Allocation RE-weighting</a></h3>
            <p class="text-sm italic">April Chen, Prajna Soni — Paper @ AAAI 2023 AI for Social Good Workshop</p>
            <p>Introduces a re-weighting approach to improve fairness in allocation settings, ensuring equitable outcomes across demographic groups.</p>
          </li>

          <li>
            <h3 class="font-semibold text-lg text-primary"><a href="https://sciencepolicyreview.org/2022/07/mitspr-191618003023/" class="text-accent hover:underline" target="_blank">Natural Language Processing: Understanding the Current Landscape</a></h3>
            <p class="text-sm italic">Medha Patki, Prajna Soni — Article @ MIT Science Policy Review V3</p>
            <p>A policy-oriented overview of contemporary trends and challenges in NLP, including ethical and governance considerations.</p>
          </li>
        </ul>

      </div>
    </div>
  </div>
</section>
